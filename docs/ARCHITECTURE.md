# Zeroth â€” Technical Architecture Document

> **A fully open-source data lakehouse â€” store, query, stream, and visualize data at any scale.**
>
> Version: 1.0 | Last Updated: February 2026

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Zeroth Architecture Overview](#2-zeroth-architecture-overview)
3. [Open-Source Component Mapping](#3-open-source-component-mapping)
4. [Storage Layer â€” MinIO](#4-storage-layer--minio)
5. [Table Format â€” Apache Iceberg + Parquet](#5-table-format--apache-iceberg--parquet)
6. [Catalog & Governance â€” Apache Polaris](#6-catalog--governance--apache-polaris)
7. [Query Engine â€” Trino](#7-query-engine--trino)
8. [Data Ingestion â€” Kafka + NiFi](#8-data-ingestion--kafka--nifi)
9. [Web UI â€” Apache Superset](#9-web-ui--apache-superset)
10. [Feature Parity Analysis](#10-feature-parity-analysis)
11. [Deployment Architecture](#11-deployment-architecture)
12. [Performance Considerations](#12-performance-considerations)
13. [Security Architecture](#13-security-architecture)
14. [Limitations & Trade-offs](#14-limitations--trade-offs)
15. [Roadmap](#15-roadmap)

---

## 1. Executive Summary

Zeroth is a fully open-source data lakehouse built on three architectural principles:

1. **Separation of storage and compute** â€” scale each independently
2. **Multi-cluster shared data** â€” concurrent workloads with zero contention
3. **Low administration overhead** â€” automated tuning, scaling, and maintenance via Kubernetes

This document describes Zeroth's architecture and the mature, production-proven technologies behind it. The stack centers on **seven pillars**:

| Pillar | Technology | Role |
|--------|-----------|------|
| **Storage** | MinIO | S3-compatible object storage |
| **Table Format** | Apache Iceberg + Parquet | ACID tables on object storage |
| **Catalog** | Apache Polaris | Metadata, RBAC, discovery |
| **Query Engine** | Trino | Distributed SQL (MPP) |
| **Streaming** | Redpanda | Kafka-compatible event streaming (C++, no JVM) |
| **Ingestion** | Apache NiFi | Visual data flow & ETL routing |
| **Web UI** | Apache Superset | SQL IDE, dashboards, data explorer |

All orchestrated on **Kubernetes** for elastic scaling.

### Current Architecture

![Zeroth Architecture Diagram](architecture-diagram.png)

---

## 2. Zeroth Architecture Overview

Zeroth's architecture has three distinct layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cloud Services Layer                      â”‚
â”‚   (Authentication, Metadata, Query Optimization, RBAC)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    VW 1  â”‚   VW 2   â”‚   VW 3   â”‚   VW 4   â”‚   ...           â”‚
â”‚  (XS)    â”‚  (S)     â”‚  (M)     â”‚  (L)     â”‚                 â”‚
â”‚          â”‚          â”‚          â”‚          â”‚  Compute Layer    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                Centralized Storage Layer                      â”‚
â”‚          (Micro-partitions in columnar format)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Principles

| Principle | Description |
|-----------|-------------|
| **Shared-nothing compute** | Each virtual warehouse (VW) is an independent MPP cluster |
| **Shared storage** | All compute clusters read/write the same data |
| **Micro-partitions** | Data stored as immutable, compressed columnar files (50â€“500 MB) |
| **Automatic clustering** | Data is automatically organized for query performance |
| **Multi-cluster warehouses** | Auto-scale by adding clusters when concurrency spikes |

---

## 3. Open-Source Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Zeroth                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Cloud Services â”‚  Compute         â”‚   Centralized                  â”‚
â”‚   Layer          â”‚  Layer           â”‚   Storage                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Apache Polaris   â”‚  Trino           â”‚  MinIO                         â”‚
â”‚ (Catalog, RBAC)  â”‚  (MPP SQL)       â”‚  (S3-compatible Object Store)  â”‚
â”‚                  â”‚                  â”‚                                â”‚
â”‚ + Apache Ranger  â”‚  on Kubernetes   â”‚  + Apache Iceberg              â”‚
â”‚ + OPA            â”‚  (auto-scaling)  â”‚  + Apache Parquet              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Storage Layer â€” MinIO

### Why MinIO?

MinIO is a **high-performance, S3-compatible object storage** system. It is the de facto standard for self-hosted object storage.

| Aspect | Details |
|--------|---------|
| **S3 Compatibility** | Full AWS S3 API compatibility â€” Iceberg, Trino, Spark all work natively |
| **Performance** | Designed for high-throughput workloads; benchmarks show performance comparable to cloud S3 |
| **Erasure Coding** | Data protection without RAID; configurable redundancy |
| **Encryption** | SSE-S3, SSE-KMS, and SSE-C encryption at rest |
| **Multi-site Replication** | Async replication across sites for DR |

### Storage Concepts

| Concept | MinIO Implementation |
|---|---|
| Internal stage | S3 bucket (e.g., `s3://warehouse/`) |
| Micro-partition files | Parquet files in bucket prefixes |
| Storage auto-scaling | MinIO cluster expansion (add nodes) |
| Cross-region replication | MinIO site replication |

### Configuration

```yaml
# MinIO deployment sizing
# Single-node dev:     1 node,  1 drive
# Production minimum:  4 nodes, 4 drives each (erasure coding)
# Large scale:         16+ nodes, NVMe drives

MINIO_ROOT_USER: admin
MINIO_ROOT_PASSWORD: <strong-password>
MINIO_VOLUMES: "/data{1...4}"        # 4 drives per node
MINIO_SERVER_URL: "https://minio.example.com"
```

### Storage Tiers (Automatic Tiering)

MinIO supports **Information Lifecycle Management (ILM)** rules to move cold data to cheaper tiers:

```json
{
  "Rules": [
    {
      "Status": "Enabled",
      "Filter": { "Prefix": "warehouse/" },
      "Transition": {
        "Days": 90,
        "StorageClass": "GLACIER"    // Move to cold tier after 90 days
      }
    }
  ]
}
```

---

## 5. Table Format â€” Apache Iceberg + Parquet

### The Critical Abstraction

Apache Iceberg is the **most important component** in this architecture. It provides the **table abstraction** that makes object storage act like a data warehouse.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Apache Iceberg              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚     Metadata Layer           â”‚  â”‚
â”‚   â”‚  â€¢ Table schema              â”‚  â”‚
â”‚   â”‚  â€¢ Partition spec            â”‚  â”‚
â”‚   â”‚  â€¢ Snapshot history          â”‚  â”‚
â”‚   â”‚  â€¢ Sort order                â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚     Manifest Lists           â”‚  â”‚
â”‚   â”‚  (pointers to manifests)     â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚     Manifest Files           â”‚  â”‚
â”‚   â”‚  (pointers to data files     â”‚  â”‚
â”‚   â”‚   + column-level stats)      â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚     Data Files (Parquet)     â”‚  â”‚
â”‚   â”‚  (columnar, compressed,      â”‚  â”‚
â”‚   â”‚   50-500 MB each)            â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Feature Overview

| Feature | Iceberg Implementation | Notes |
|---|---|---|
| **Micro-partitions** | Parquet data files | Same concept: immutable, columnar, ~100-500 MB |
| **Automatic clustering** | Sort orders + `optimize` | Manual trigger, but same effect |
| **Time Travel** | Snapshot-based time travel | Query any previous version via snapshot ID or timestamp |
| **Zero-copy Cloning** | Branching (via catalog) | New metadata pointer, no data copy |
| **Schema Evolution** | Native schema evolution | Add, rename, reorder, widen columns |
| **Partition Evolution** | Native partition evolution | Change partitioning without data rewrite |
| **ACID Transactions** | Optimistic concurrency | Serializable isolation via metadata swap |
| **Data Retention** | Snapshot expiration | `expire_snapshots` procedure |

### Why Iceberg Over Delta Lake or Hudi?

| Criteria | Iceberg | Delta Lake | Hudi |
|----------|---------|------------|------|
| **Engine independence** | â­ Any engine | Spark-centric | Spark-centric |
| **Partition evolution** | â­ Yes | No | No |
| **Hidden partitioning** | â­ Yes | No | No |
| **REST Catalog standard** | â­ Yes | Unity Catalog | No |
| **Industry adoption** | â­ Used natively by major cloud data warehouses | No | No |
| **Community momentum** | â­ Fastest growing | Large | Moderate |

### Parquet Format Details

Apache Parquet provides the **physical columnar storage**:

- **Columnar layout** â€” Read only the columns you need
- **Predicate pushdown** â€” Skip row groups that don't match filters
- **Dictionary encoding** â€” Compress low-cardinality columns
- **Snappy/ZSTD compression** â€” Typically 3-10x compression ratios
- **Row group stats** â€” Min/max values for partition pruning

```
Parquet File Structure:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Row Group 1          â”‚
â”‚  â”œâ”€ Column A chunk   â”‚  â† Dictionary + RLE encoded
â”‚  â”œâ”€ Column B chunk   â”‚  â† Snappy compressed
â”‚  â””â”€ Column C chunk   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Row Group 2          â”‚
â”‚  â”œâ”€ Column A chunk   â”‚
â”‚  â”œâ”€ Column B chunk   â”‚
â”‚  â””â”€ Column C chunk   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Footer               â”‚
â”‚  â”œâ”€ Schema           â”‚
â”‚  â”œâ”€ Row group stats  â”‚  â† Min/max for predicate pushdown
â”‚  â””â”€ Column stats     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Catalog & Governance â€” Apache Polaris

### Why Polaris?

Apache Polaris was **originally developed as an internal Iceberg catalog** for a major cloud data platform, then open-sourced and donated to the Apache Software Foundation. It is the **reference implementation** of the Iceberg REST Catalog specification.

### Polaris Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Apache Polaris                  â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚          REST Catalog API               â”‚ â”‚
â”‚  â”‚  (Iceberg REST Spec compliant)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚             â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     Catalog Management                  â”‚ â”‚
â”‚  â”‚  â€¢ Namespaces (â‰ˆ databases)         â”‚ â”‚
â”‚  â”‚  â€¢ Tables                               â”‚ â”‚
â”‚  â”‚  â€¢ Views                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚             â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     Access Control (RBAC)               â”‚ â”‚
â”‚  â”‚  â€¢ Catalog roles                        â”‚ â”‚
â”‚  â”‚  â€¢ Principal roles                      â”‚ â”‚
â”‚  â”‚  â€¢ Privilege grants                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚             â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     Storage Profiles                    â”‚ â”‚
â”‚  â”‚  â€¢ S3, GCS, ADLS, MinIO                 â”‚ â”‚
â”‚  â”‚  â€¢ Vended credentials                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mapping to Data Platform Concepts

| Data Platform Concept | Polaris Equivalent |
|---|---|
| Account | Catalog (top-level container) |
| Database | Namespace |
| Schema | Nested namespace |
| Table | Iceberg table |
| Role | Catalog role + Principal role |
| GRANT privilege | Privilege grant API |
| Data sharing | Cross-catalog table access |

### RBAC Model

Polaris provides **enterprise-grade RBAC** with two role types:

```
Principal Roles (WHO)          Catalog Roles (WHAT)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ data_engineer  â”‚â”€â”€grantsâ”€â”€â–¶  â”‚ warehouse_admin   â”‚â”€â”€privilegesâ”€â”€â–¶ Tables
â”‚ analyst        â”‚â”€â”€grantsâ”€â”€â–¶  â”‚ read_only         â”‚â”€â”€privilegesâ”€â”€â–¶ Namespaces
â”‚ ml_engineer    â”‚â”€â”€grantsâ”€â”€â–¶  â”‚ ml_read_write     â”‚â”€â”€privilegesâ”€â”€â–¶ Views
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Supported privileges:**
- `CATALOG_MANAGE_CONTENT` â€” Full DDL/DML
- `TABLE_READ_DATA` â€” SELECT on tables
- `TABLE_WRITE_DATA` â€” INSERT/UPDATE/DELETE
- `TABLE_CREATE` â€” Create new tables
- `NAMESPACE_CREATE` â€” Create namespaces
- `VIEW_CREATE` â€” Create views

### Polaris vs. Nessie â€” When to Use Each

| Use Case | Polaris | Nessie |
|----------|---------|--------|
| Standard Iceberg catalog | â­ Reference implementation | âœ… Supported |
| Built-in RBAC | â­ Native | âŒ External needed |
| Git-like branching | âŒ Not supported | â­ Core feature |
| CI/CD for data | âŒ Not supported | â­ Branch â†’ test â†’ merge |
| Enterprise parity | â­ Highest (from major cloud vendor) | âœ… Partial |
| Multi-engine support | â­ REST standard | â­ REST + native |

> **Recommendation:** Use **Polaris as primary catalog** for production governance. Add **Nessie** alongside it if you need data versioning / branching workflows (e.g., staging â†’ production promotions).

---

## 7. Query Engine â€” Trino

### Why Trino?

Trino (formerly PrestoSQL) is an **open-source distributed SQL query engine** designed for interactive analytics. In Zeroth, Trino clusters serve as the compute layer.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Trino Cluster                     â”‚
â”‚            (â‰ˆ Zeroth Compute Cluster)              â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Coordinator                     â”‚  â”‚
â”‚  â”‚  â€¢ Query parsing & planning                  â”‚  â”‚
â”‚  â”‚  â€¢ Cost-based optimizer                      â”‚  â”‚
â”‚  â”‚  â€¢ Query scheduling                          â”‚  â”‚
â”‚  â”‚  â€¢ Web UI (port 8080)                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚  distributes work                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Workers (N nodes)               â”‚  â”‚
â”‚  â”‚  â€¢ Parallel data processing                  â”‚  â”‚
â”‚  â”‚  â€¢ In-memory pipeline execution              â”‚  â”‚
â”‚  â”‚  â€¢ Local caching (Alluxio optional)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Compute Layer Concepts

| Concept | Trino Implementation |
|---|---|
| Virtual Warehouse (XSâ€“6XL) | Trino cluster (configure worker count) |
| Multi-cluster warehouse | Multiple Trino clusters on K8s |
| Auto-suspend | K8s HPA scale-to-zero (KEDA) |
| Auto-resume | K8s KEDA trigger on query arrival |
| Query result caching | Trino result caching + Alluxio data caching |
| Resource monitor | Trino resource groups |

### Virtual Warehouse Sizing Equivalent

| Cluster Size | Trino Configuration | Workers | Memory/Worker |
|---|---|---|---|
| X-Small | trino-xs | 1 | 8 GB |
| Small | trino-sm | 2 | 16 GB |
| Medium | trino-md | 4 | 32 GB |
| Large | trino-lg | 8 | 64 GB |
| X-Large | trino-xl | 16 | 64 GB |
| 2X-Large | trino-2xl | 32 | 128 GB |

### Concurrency & Resource Groups

Trino's **Resource Groups** provide concurrency controls:

```json
{
  "rootGroups": [
    {
      "name": "etl",
      "maxQueued": 100,
      "hardConcurrencyLimit": 10,
      "softMemoryLimit": "60%",
      "schedulingPolicy": "fair"
    },
    {
      "name": "analytics",
      "maxQueued": 500,
      "hardConcurrencyLimit": 50,
      "softMemoryLimit": "30%",
      "schedulingPolicy": "fair"
    },
    {
      "name": "adhoc",
      "maxQueued": 50,
      "hardConcurrencyLimit": 5,
      "softMemoryLimit": "10%"
    }
  ]
}
```

### Connectors (Federated Query â€” Bonus!)

Trino can also query **multiple data sources** simultaneously (federated queries):

```sql
-- Query across Iceberg, PostgreSQL, and MongoDB in one query!
SELECT
    i.user_id,
    i.total_purchases,
    p.email,
    m.preferences
FROM iceberg.analytics.purchases i
JOIN postgresql.users.profiles p ON i.user_id = p.id
JOIN mongodb.app.user_prefs m ON i.user_id = m.user_id;
```

---

## 8. Data Ingestion â€” Redpanda + NiFi

### The Ingestion Problem

For automatic data ingestion â€” where events or files are continuously loaded into Iceberg tables â€” Zeroth uses two components:

- **Redpanda** â€” Kafka-compatible event streaming in C++ (handles real-time data streams)
- **Apache NiFi** â€” Visual data flow engine (handles automatic ingestion pipelines)

### Pipeline Architecture: Redpanda â†’ NiFi â†’ Iceberg

```
Data Sources                Streaming Buffer          Data Flow Engine           Lakehouse
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€

APIs          â”€â”                                     â”Œâ”€ ConvertRecord â”€â”
Databases     â”€â”¤                                     â”‚  (JSONâ†’Parquet) â”‚
Files (S3)    â”€â”¼â”€â”€â–¶ Redpanda Topics â”€â”€â–¶ Apache NiFi â”€â”¤                 â”œâ”€â”€â–¶ Iceberg Tables
IoT Sensors   â”€â”¤    (ConsumeKafka)     (visual UI)   â”‚  RouteOnAttr    â”‚    (on MinIO)
Log Streams   â”€â”˜                                     â”‚  ValidateRecord â”‚
                                                     â””â”€â”€ PutIceberg  â”€â”€â”˜
```

### Why Redpanda? (Streaming Buffer)

| Aspect | Details |
|--------|---------|
| **Role** | High-throughput event buffer between producers and NiFi |
| **Kafka compatible** | 100% Kafka wire-protocol compatible â€” all Kafka clients work unchanged |
| **No JVM** | Written in C++, uses ~256 MB RAM vs Kafka's ~1-2 GB |
| **Replay** | Consumers can re-read historical events (change data capture equivalent) |
| **Decoupling** | Multiple consumers (NiFi, Spark, Flink) can read the same topics |
| **Throughput** | Millions of events/sec per cluster |
| **Retention** | Configurable retention (7 days default, up to infinite) |

### Why NiFi? (Data Flow Engine)

| Aspect | Details |
|--------|---------|
| **Role** | Visual, drag-and-drop data routing and transformation |
| **Auto-ingestion** | Consumes from Redpanda, transforms, writes to Iceberg via Trino |
| **300+ processors** | Built-in connectors for files, databases, APIs, cloud services |
| **PutDatabaseRecord** | JDBC-based Iceberg writer via Trino (PutIceberg has MinIO compatibility issues) |
| **Backpressure** | Built-in per-connection backpressure (prevents data loss) |
| **Visual monitoring** | Real-time flow monitoring, error handling, provenance tracking |
| **No code required** | Data engineers configure pipelines via UI, not code |

### NiFi Flow: Redpanda â†’ Trino â†’ Iceberg

The working NiFi pipeline uses **PutDatabaseRecord** with Trino JDBC to write to Iceberg tables. NiFi's `PutIceberg` processor has a known NPE bug with non-AWS S3 endpoints like MinIO (`writer is null`), so data is routed through Trino instead:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ConsumeKafka â”‚â”€â”€â–¶ â”‚ PutDatabaseRecord â”‚
â”‚              â”‚    â”‚                   â”‚
â”‚ bootstrap:   â”‚    â”‚ JDBC: Trino       â”‚
â”‚ redpanda:9092â”‚    â”‚ Table: events     â”‚
â”‚ topic:       â”‚    â”‚ Auto-Commit: true â”‚
â”‚ raw-events   â”‚    â”‚ Reader: JSON      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ INSERT via JDBC
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Trino (SQL)      â”‚
                    â”‚  â†’ Iceberg format â”‚
                    â”‚  â†’ MinIO (S3)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Why not PutIceberg?** NiFi 2.8.0's PutIceberg produces a `NullPointerException: writer is null` when writing to MinIO. The Parquet DataWriter factory fails silently for non-AWS S3 endpoints. Routing writes through Trino's JDBC driver bypasses this entirely since Trino already has working MinIO integration.

### Key NiFi Processors for Zeroth

| Processor | Purpose |
|-----------|---------|
| `ConsumeKafka` | Read events from Redpanda topics (Kafka wire-compatible) |
| `PutDatabaseRecord` | Insert records into Iceberg tables via Trino JDBC |
| `ConvertRecord` | Transform between formats (JSON/Avro/CSV) |
| `ValidateRecord` | Schema validation before writing |
| `RouteOnAttribute` | Route events to different Iceberg tables by type |
| `PutS3Object` | Write to MinIO (dead letter queue, raw archives) |
| `QueryRecord` | SQL-like filtering and transformation within NiFi |

### Redpanda + NiFi vs. Flink

| Aspect | Redpanda â†’ **NiFi** â†’ Iceberg | Redpanda â†’ **Flink** â†’ Iceberg |
|--------|-------------------------------|-------------------------------|
| **Setup** | ğŸŸ¢ Visual, drag-and-drop | ğŸ”´ Java/SQL code |
| **Learning curve** | ğŸŸ¢ Low | ğŸ”´ High |
| **Transformations** | ğŸŸ¢ 300+ built-in processors | ğŸŸ¢ Full SQL/Java |
| **Windowed aggregations** | ğŸ”´ Not supported | ğŸŸ¢ Native (tumbling, sliding) |
| **Exactly-once** | ğŸŸ¡ At-least-once | ğŸŸ¢ Exactly-once |
| **Throughput** | ğŸŸ¡ ~100K events/sec | ğŸŸ¢ Millions/sec |
| **Monitoring** | ğŸŸ¢ Built-in visual UI | ğŸ”´ Separate dashboards |
| **Best for** | ETL, file routing, moderate scale | Real-time analytics, high scale |

> **Recommendation:** Start with **Redpanda â†’ NiFi â†’ Iceberg** for most workloads. Add Flink only if you need windowed stream aggregations or exactly-once at extreme scale.

---

## 9. Web UI â€” Apache Superset

### Why Superset?

Apache Superset is an **open-source BI platform** that serves as Zeroth's web console. It provides a SQL editor, rich dashboards, and data exploration â€” all connected to Trino.

### Web Console Features

| Feature | Superset Implementation |
|---|---|
| **Worksheets** (SQL editor) | **SQL Lab** â€” full SQL IDE with auto-complete, query history |
| **Dashboards** | **Dashboards** â€” 50+ chart types, filters, drill-down |
| **Data Explorer** | **Dataset browser** â€” browse schemas, tables, columns |
| **Query History** | **Query history** â€” past queries, re-run, share |
| **Charts & Viz** | **Explore** â€” bar, line, pie, heatmap, geospatial, etc. |
| **Sharing** | **Dashboard sharing** â€” public links, embedded iframes |
| **RBAC** | **Role-based access** â€” row-level security, dataset permissions |
| **Alerts** | **Alerts & Reports** â€” scheduled queries, email/Slack alerts |

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Apache Superset                        â”‚
â”‚          (â‰ˆ Zeroth Web Console)                     â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  SQL Lab     â”‚  â”‚ Dashboardsâ”‚  â”‚ Data Explorerâ”‚  â”‚
â”‚  â”‚  (Worksheets)â”‚  â”‚ (Charts)  â”‚  â”‚ (Datasets)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                          â”‚                          â”‚
â”‚                          â”‚                          â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚               â”‚  SQLAlchemy + Trino    â”‚            â”‚
â”‚               â”‚  (trino://trino:8080)  â”‚            â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ 
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚    Trino Query Engine   â”‚
             â”‚    (Iceberg on MinIO)   â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Connecting Superset to Trino

Superset connects to Trino via the **SQLAlchemy Trino driver**:

```
# Superset database connection string
trino://trino@trino:8080/iceberg
```

Once connected, all Iceberg schemas and tables are browsable in Superset's SQL Lab, and queryable for dashboards.

### Key Superset Features for Zeroth

| Feature | Description |
|---------|-----------|
| **SQL Lab** | Full SQL IDE with syntax highlighting, auto-complete, and results grid |
| **50+ chart types** | Bar, line, pie, treemap, heatmap, geospatial, time-series, etc. |
| **Dashboard filters** | Cross-filter across charts, date range pickers, dropdown filters |
| **Jinja templating** | Dynamic SQL with `{{ current_username() }}`, date macros |
| **Row-level security** | Restrict data visibility per role (row access policies) |
| **Alerts & Reports** | Schedule queries, get notified on thresholds (email/Slack) |
| **Embedded analytics** | Embed dashboards via iframe in your own apps |
| **Caching** | Redis-backed query caching for faster dashboard loads |

---

## 10. Feature Parity Analysis

### Full Feature Comparison

| Feature | Status | Technology | Notes |
|---|:---:|---|---|
| **Separation of storage/compute** | âœ… Full | MinIO + Trino | None |
| **Columnar storage** | âœ… Full | Apache Parquet | None |
| **ACID transactions** | âœ… Full | Apache Iceberg | None |
| **Time Travel** | âœ… Full | Iceberg snapshots | None |
| **Zero-copy cloning** | âœ… Full | Iceberg branches/tags | None |
| **Schema evolution** | âœ… Full | Iceberg native | None |
| **Partition evolution** | â­ Advanced | Iceberg native | Change partitioning without data rewrite |
| **SQL engine (ANSI SQL)** | âœ… Full | Trino | Minor syntax diffs |
| **RBAC** | âœ… Full | Polaris | None |
| **Concurrency scaling** | âœ… Full | K8s + multi-cluster Trino | Requires K8s expertise |
| **Auto-suspend/resume** | âš ï¸ Partial | KEDA + K8s HPA | Not as seamless |
| **Semi-structured data** | âœ… Full | Trino JSON functions | None |
| **Secure data sharing** | âš ï¸ Partial | Polaris cross-catalog | Less polished |
| **Streams & Tasks** | âœ… Full | Redpanda (streaming) + NiFi (routing) | Visual pipeline builder |
| **Auto-ingest** | âœ… Full | Redpanda â†’ NiFi â†’ PutIceberg | NiFi drag-and-drop UI |
| **Query result caching** | âš ï¸ Partial | Trino + Alluxio | Not as transparent |
| **Materialized views** | âŒ Gap | Not native in Iceberg/Trino | Use dbt for models |
| **UDFs (Java/Python)** | âš ï¸ Partial | Trino UDFs (Java) | No Python UDFs |
| **Search optimization** | âš ï¸ Partial | Iceberg file-level stats | Less granular |
| **Governance (masking, tags)** | âš ï¸ Partial | Ranger + custom | More manual |

### What's Better in the Open-Source Stack

| Feature | Advantage |
|---------|-----------|
| **Partition evolution** | Change partitioning without data rewrite |
| **Federated queries** | Query Postgres, MongoDB, Kafka, etc. alongside Iceberg |
| **Engine flexibility** | Use Spark, Flink, DuckDB, or Trino on the same tables |
| **Vendor independence** | No lock-in; portable Iceberg tables |
| **Cost** | No vendor license fees; pay only for infrastructure |

---

## 11. Deployment Architecture

### Development (Docker Compose â€” 14 services, 3 profiles)

```
docker-compose.yml (3 profiles)

  Profile: core
  â”œâ”€â”€ MinIO              (S3 storage)       â†’ localhost:9000 / :9001 (console)
  â”œâ”€â”€ minio-init         (bucket creation)  â†’ creates warehouse + iceberg buckets
  â”œâ”€â”€ PostgreSQL         (Polaris metadata) â†’ localhost:5432
  â”œâ”€â”€ Polaris            (catalog + RBAC)   â†’ localhost:8181 / :8182 (mgmt)
  â””â”€â”€ Trino              (query engine)     â†’ localhost:8080

  Profile: bootstrap-db / bootstrap (run once)
  â”œâ”€â”€ polaris-db-bootstrap   (schema migration)
  â””â”€â”€ polaris-bootstrap      (catalog, roles, grants)

  Profile: ingestion
  â”œâ”€â”€ Redpanda           (streaming, C++)   â†’ localhost:9092 / :8082 (HTTP)
  â”œâ”€â”€ Redpanda Console   (streaming UI)     â†’ localhost:8084
  â””â”€â”€ NiFi               (data flow)        â†’ https://localhost:8443

  Profile: ui
  â”œâ”€â”€ Redis              (cache + broker)   â†’ localhost:6379
  â”œâ”€â”€ Redis Commander    (Redis UI)         â†’ localhost:8081
  â”œâ”€â”€ Superset DB        (PostgreSQL)       â†’ localhost:5433
  â”œâ”€â”€ Superset           (BI / SQL Lab)     â†’ localhost:8088
  â”œâ”€â”€ Superset Worker    (Celery async)     â†’ background
  â””â”€â”€ Superset Flower    (task monitor)     â†’ localhost:5555
```

See [`docker/docker-compose.yml`](../docker/docker-compose.yml) for the full configuration.

### Production (Kubernetes)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Kubernetes Cluster                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Namespace: data-platform                                 â”‚   â”‚
â”‚  â”‚                                                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚   Polaris    â”‚  â”‚  Trino ETL  â”‚  â”‚ Trino Analytics â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  (catalog)   â”‚  â”‚  Cluster    â”‚  â”‚   Cluster       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Deployment  â”‚  â”‚  (VW #1)    â”‚  â”‚   (VW #2)       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  replicas: 2 â”‚  â”‚  workers: 8 â”‚  â”‚  workers: 4     â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚         â”‚                 â”‚                  â”‚            â”‚   â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚
â”‚  â”‚                           â”‚                               â”‚   â”‚
â”‚  â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                        â”‚   â”‚
â”‚  â”‚                    â”‚    MinIO     â”‚                        â”‚   â”‚
â”‚  â”‚                    â”‚  StatefulSet â”‚                        â”‚   â”‚
â”‚  â”‚                    â”‚  nodes: 4    â”‚                        â”‚   â”‚
â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Namespace: monitoring                                    â”‚   â”‚
â”‚  â”‚  Prometheus + Grafana                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Auto-Scaling with KEDA

```yaml
# Scale Trino workers based on pending queries (auto-scale)
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: trino-worker-scaler
spec:
  scaleTargetRef:
    name: trino-worker
  minReplicaCount: 0        # Scale to zero when idle (auto-suspend!)
  maxReplicaCount: 32       # Max workers
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus:9090
        metricName: trino_queued_queries
        threshold: "5"      # Scale up when >5 queries queued
        query: |
          trino_queries_queued_total{cluster="analytics"}
```

---

## 12. Performance Considerations

### Query Performance Optimization

| Technique | How | Equivalent Concept |
|-----------|-----|---------------------|
| **Partition pruning** | Iceberg hidden partitioning + manifest stats | Micro-partition pruning |
| **Predicate pushdown** | Parquet row group min/max + Iceberg file stats | Automatic pruning |
| **Columnar projection** | Read only needed columns from Parquet | Columnar scan |
| **Data compaction** | `ALTER TABLE ... EXECUTE optimize` | Automatic clustering |
| **Sort ordering** | Iceberg sort orders (z-order supported) | Cluster keys |
| **Caching** | Alluxio (data) + Trino result cache | Result cache + local disk |

### Benchmarks (Approximate)

Benchmarks vary by hardware, data, and query patterns. General guidance:

| Workload | Expected Performance |
|----------|-----------------------------------|
| Simple aggregations (scan-heavy) | **80-100%** â€” Trino + Parquet is very efficient |
| Complex joins (shuffle-heavy) | **60-80%** of managed cloud warehouses â€” their optimizers have years of tuning |
| High concurrency (100+ users) | **70-90%** â€” requires proper K8s tuning |
| Semi-structured (JSON) | **70-85%** of managed warehouses â€” VARIANT types are deeply optimized |
| Time travel queries | **95-100%** â€” Iceberg snapshots are very efficient |

### Caching Architecture

```
Query Flow:
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Client â”€â”€â–¶ Trino Coordinator â”€â”€â–¶ Check â”€â”¤ Result Cache â”‚ â”€â”€â–¶ Return cached result
                                        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚ miss
                                        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚ Alluxio      â”‚ â”€â”€â–¶ Read from local SSD
                                        â”‚ (data cache) â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚ miss
                                        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚ MinIO (S3)   â”‚ â”€â”€â–¶ Read from object storage
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 13. Security Architecture

### Defense in Depth

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: Network Security                              â”‚
â”‚  â€¢ K8s Network Policies                                 â”‚
â”‚  â€¢ TLS everywhere (mTLS between services)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: Authentication                                â”‚
â”‚  â€¢ OAuth 2.0 / OIDC (Keycloak)                          â”‚
â”‚  â€¢ LDAP integration                                     â”‚
â”‚  â€¢ Service-to-service mTLS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 3: Authorization (RBAC)                          â”‚
â”‚  â€¢ Polaris: Catalog-level RBAC                          â”‚
â”‚  â€¢ Apache Ranger: Fine-grained table/column policies    â”‚
â”‚  â€¢ OPA: Policy-as-code for custom rules                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 4: Data Protection                               â”‚
â”‚  â€¢ MinIO SSE (encryption at rest)                       â”‚
â”‚  â€¢ TLS (encryption in transit)                          â”‚
â”‚  â€¢ Column-level masking (via Ranger)                    â”‚
â”‚  â€¢ Row-level filtering (via Ranger)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 5: Auditing                                      â”‚
â”‚  â€¢ Trino query audit log                                â”‚
â”‚  â€¢ Polaris access audit log                             â”‚
â”‚  â€¢ MinIO access logs â†’ centralized logging              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Column-Level Security (via Ranger)

```xml
<!-- Apache Ranger Policy: Mask SSN column for analysts -->
<policy>
  <name>mask-ssn</name>
  <resources>
    <database>analytics</database>
    <table>customers</table>
    <column>ssn</column>
  </resources>
  <dataMaskPolicyItems>
    <roles>analyst</roles>
    <dataMaskType>MASK_SHOW_LAST_4</dataMaskType>
  </dataMaskPolicyItems>
</policy>
```

---

## 14. Limitations & Trade-offs

### Known Gaps vs. Managed Cloud Warehouses

| Area | Gap | Mitigation |
|------|-----|------------|
| **Zero-ops** | Open-source requires K8s expertise and operational overhead | Use managed K8s (EKS/GKE) + Helm charts + GitOps |
| **Query optimizer** | Managed warehouse optimizers have years of tuning on customer workloads | Trino's CBO is improving; contribute upstream |
| **Auto-clustering** | Managed warehouses automatically re-cluster data | Schedule `optimize` jobs via Airflow/cron |
| **Materialized views** | No native MVs in Trino + Iceberg | Use dbt incremental models |
| **Python UDFs** | Trino only supports Java UDFs | Use Spark for Python-heavy workloads |
| **Marketplace** | No data marketplace | Build custom using Polaris cross-catalog sharing |
| **Support & SLA** | Community support only | Starburst (commercial Trino) offers enterprise support |

### Operational Complexity

> [!WARNING]
> This stack trades **managed simplicity** for **full control and cost savings**. You will need:
> - Kubernetes expertise (or managed K8s)
> - Monitoring and alerting (Prometheus + Grafana)
> - On-call for storage, compute, and catalog components
> - Performance tuning knowledge

### When NOT to Use This Stack

- **Small team (< 5 engineers)** with no K8s experience â†’ Consider a managed cloud warehouse
- **Need it running today** â†’ Managed services are faster to deploy
- **Compliance requires vendor SLAs** â†’ Use Starburst (commercial Trino) or Tabular (commercial Iceberg)

---

## 15. Roadmap

### Phase 1: Foundation âœ…
- [x] MinIO storage cluster with path-style S3 access
- [x] Apache Polaris catalog with PostgreSQL persistence
- [x] Polaris bootstrap automation (catalog, roles, privileges)
- [x] Single Trino cluster with Native S3 + OAuth2
- [x] Basic SQL queries on Iceberg tables (schema â†’ table â†’ insert â†’ select)
- [x] Docker Compose local dev environment with phased startup
- [x] PyIceberg Python ingestion script

### Phase 2: Data Ingestion + BI âœ…
- [x] Deploy Redpanda (Kafka-compatible, C++, no JVM)
- [x] Deploy Apache NiFi (HTTPS, single-user auth)
- [x] Deploy Superset with Celery workers, Flower, Redis
- [ ] Build Redpanda â†’ NiFi â†’ Iceberg pipeline
- [ ] Configure ConsumeKafka + PutIceberg flow
- [ ] Set up dead letter queue for failed records
- [ ] Create sample Superset dashboard with Trino connection

### Phase 3: Governance (Weeks 5â€“6)
- [x] Configure Polaris RBAC (catalog roles, principal roles)
- [ ] Set up authentication (OAuth/OIDC)
- [ ] Column-level masking with Ranger
- [ ] Audit logging

### Phase 4: Production (Weeks 7â€“10)
- [ ] Kubernetes deployment (Helm charts)
- [ ] Multi-cluster Trino (ETL + Analytics warehouses)
- [ ] Auto-scaling with KEDA
- [ ] Monitoring (Prometheus + Grafana dashboards)
- [ ] Alluxio caching layer

### Phase 5: Advanced (Weeks 11â€“14)
- [ ] dbt integration for transformations
- [ ] Cross-catalog data sharing via Polaris
- [ ] Disaster recovery (MinIO site replication)
- [ ] Cost management and chargeback
- [ ] Add Flink for windowed stream processing (if needed)

---

## References

- [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)
- [Apache Polaris (Incubating)](https://polaris.apache.org/)
- [Trino Documentation](https://trino.io/docs/current/)
- [MinIO Documentation](https://min.io/docs/)
- [Apache Kafka](https://kafka.apache.org/documentation/)
- [Apache NiFi Documentation](https://nifi.apache.org/docs.html)
- [Apache Parquet](https://parquet.apache.org/)
- [KEDA â€” Kubernetes Event-driven Autoscaling](https://keda.sh/)
- [Apache Ranger](https://ranger.apache.org/)
- [Data Warehouse Architecture Concepts](https://docs.snowflake.com/en/user-guide/intro-key-concepts)

---

*Document generated as part of the Zeroth project â€” Last updated: February 28, 2026*
